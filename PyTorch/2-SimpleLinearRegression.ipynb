{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3647b5a9-a216-475f-a69f-e676eec2962c",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f7ac1fd-258c-4537-99f5-db1e84e80cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5359a100-6a77-451e-941c-53ddbe01236c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sat_sum</th>\n",
       "      <th>hs_gpa</th>\n",
       "      <th>fy_gpa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>508</td>\n",
       "      <td>3.40</td>\n",
       "      <td>3.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>488</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>464</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>380</td>\n",
       "      <td>3.75</td>\n",
       "      <td>2.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>428</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sat_sum  hs_gpa  fy_gpa\n",
       "0      508    3.40    3.18\n",
       "1      488    4.00    3.33\n",
       "2      464    3.75    3.25\n",
       "3      380    3.75    2.42\n",
       "4      428    4.00    2.63"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data\n",
    "data = pd.read_csv('Prodigy University Dataset.csv')\n",
    "# Split the data into features (X) and target (y)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc11d76-7acc-444b-9134-fd591c871b1a",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25876264-ce77-47b2-aa53-1367e33e510b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n",
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Converting data to numpy so that we can build tensor\n",
    "X = data[['sat_sum', 'hs_gpa']].values\n",
    "# reshape the fy_gpa into a 2D array with [data_size] rows and 1 column\n",
    "y = data['fy_gpa'].values.reshape(-1, 1)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32ae64ce-3f71-4434-8127-3868e77f5263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1d63ebc-a244-4980-8de6-0c01a4b533bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normalize the features so that it is easier to train the data\n",
    "scaler = StandardScaler()\n",
    "X_train= scaler.fit_transform(X_train)\n",
    "X_test= scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ec6756c-09d1-40c2-ab1f-9d1f526ae116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15b85d0e-c798-42e9-b51f-af42149c8ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Convert numpy to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea47c7c6-c974-4865-91ce-4d30adee87fd",
   "metadata": {},
   "source": [
    "### Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc1dc6b2-b1ca-4fb1-83f2-14cb6e5d313a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef42a4a7-53cb-4c68-a09e-a240246881c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building model with 2 neurons\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 2), # 2 input features, 1 hidden layer with 2 neurons\n",
    "    nn.Sigmoid(), \n",
    "    nn.Linear(2, 1) # output layer with single neuron output\n",
    ")\n",
    "\n",
    "# We use sigmoid activation function for hidden layer and linear activation function for output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63250876-15fe-4f94-abbb-aa93941ce0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Propagation\n",
    "preds = model(X_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "872467f7-699b-4d20-9be5-c3f4fe2f310f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0580],\n",
       "        [0.0892],\n",
       "        [0.2837],\n",
       "        [0.1818],\n",
       "        [0.2139]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a86cddaf-afa6-4dca-995d-187b8427a2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d425d597-4d18-4d3e-871e-af0105c8467e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.4397, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calculating Loss\n",
    "criterion = MSELoss()\n",
    "loss = criterion(preds, y_train_tensor)\n",
    "print(loss)\n",
    "# very high loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d539875-adad-4601-bc77-84e47be2a239",
   "metadata": {},
   "source": [
    "# Optimization and Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fc66e59-7555-4503-8951-ef2e3537c8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9ec475a-59af-4dc6-b88c-45c5171e092b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagate\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cd501a8-2248-416f-a559-82139065963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the updated weights to the model\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c1b203-6495-4011-9799-b29507207b73",
   "metadata": {},
   "source": [
    "This was done just once but we need to do this several times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aedebfcb-6609-41a4-8b40-cc33548b6ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7d4385d-567e-49e0-bc06-79afef18969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d5ea21-4d62-42a2-8adc-f6e6c25faad7",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25c67972-abc7-4b8d-8604-d85183391300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.7061, Test Loss: 0.7871\n",
      "Epoch 2: Train Loss: 0.6283, Test Loss: 0.6926\n",
      "Epoch 3: Train Loss: 0.6004, Test Loss: 0.6606\n",
      "Epoch 4: Train Loss: 0.5802, Test Loss: 0.6410\n",
      "Epoch 5: Train Loss: 0.5665, Test Loss: 0.6281\n",
      "Epoch 6: Train Loss: 0.5560, Test Loss: 0.6153\n",
      "Epoch 7: Train Loss: 0.5483, Test Loss: 0.6069\n",
      "Epoch 8: Train Loss: 0.5421, Test Loss: 0.6001\n",
      "Epoch 9: Train Loss: 0.5367, Test Loss: 0.5935\n",
      "Epoch 10: Train Loss: 0.5312, Test Loss: 0.5873\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "# Execute the training loop\n",
    "for epoch in range(10): # 10 epochs\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        pred = model(X_batch)\n",
    "        loss = criterion(pred, y_batch)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad() #reset gradient set in previous step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = criterion(model(X_train_tensor), y_train_tensor).item()\n",
    "    # print(epoch,': ', train_loss)\n",
    "    test_loss = criterion(model(X_test_tensor), y_test_tensor).item()\n",
    "    print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa0a4ce-2422-4270-9672-1f6c971dfb50",
   "metadata": {},
   "source": [
    "We see a reduction in train and test loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c59e6adb-016e-4f04-839e-dccfb04862dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.4711],\n",
       "        [2.4711],\n",
       "        [2.4699],\n",
       "        [2.4758],\n",
       "        [2.4672]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at predictions\n",
    "model(X_train_tensor)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf5f8b8-004f-424f-94e8-4b72570b1b23",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c13a0683-546d-4536-b4a0-74132e8de935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialising model weights\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(2, 1)\n",
    ")\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "886092e1-34b7-406e-8448-d7c0877e1122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)\n",
    "# This is what we will set the batch size as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22c8e87e-685d-430a-8c05-947012743fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: Train Loss: 3.4342, Test Loss: 3.6353\n",
      "Epoch 200: Train Loss: 2.1002, Test Loss: 2.2633\n",
      "Epoch 300: Train Loss: 1.3390, Test Loss: 1.4731\n",
      "Epoch 400: Train Loss: 0.9103, Test Loss: 1.0225\n",
      "Epoch 500: Train Loss: 0.6715, Test Loss: 0.7673\n",
      "Epoch 600: Train Loss: 0.5394, Test Loss: 0.6231\n",
      "Epoch 700: Train Loss: 0.4665, Test Loss: 0.5413\n",
      "Epoch 800: Train Loss: 0.4259, Test Loss: 0.4943\n",
      "Epoch 900: Train Loss: 0.4030, Test Loss: 0.4667\n",
      "Epoch 1000: Train Loss: 0.3896, Test Loss: 0.4500\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=800, shuffle=True) #800 is the number of samples in train set\n",
    "# Execute the training loop\n",
    "for epoch in range(1000): # increasing the epochs for effective training\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        pred = model(X_batch)\n",
    "        loss = criterion(pred, y_batch)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0: # printing after every 100 epochs\n",
    "        train_loss = criterion(model(X_train_tensor), y_train_tensor).item()\n",
    "        # print(epoch,': ', train_loss)\n",
    "        test_loss = criterion(model(X_test_tensor), y_test_tensor).item()\n",
    "        print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2097b2d0-2482-48f4-9334-17706649c7df",
   "metadata": {},
   "source": [
    "### Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2bc7d621-ee85-4ea8-8ec3-513006067bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialising model weights\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(2, 1)\n",
    ")\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56175308-4862-4630-b96e-a5628e8f502c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: Train Loss: 1.0300, Test Loss: 1.1382\n",
      "Epoch 100: Train Loss: 0.7152, Test Loss: 0.7870\n",
      "Epoch 150: Train Loss: 0.6046, Test Loss: 0.6667\n",
      "Epoch 200: Train Loss: 0.5249, Test Loss: 0.5825\n",
      "Epoch 250: Train Loss: 0.4678, Test Loss: 0.5230\n",
      "Epoch 300: Train Loss: 0.4282, Test Loss: 0.4824\n",
      "Epoch 350: Train Loss: 0.4011, Test Loss: 0.4551\n",
      "Epoch 400: Train Loss: 0.3827, Test Loss: 0.4368\n",
      "Epoch 450: Train Loss: 0.3702, Test Loss: 0.4249\n",
      "Epoch 500: Train Loss: 0.3616, Test Loss: 0.4169\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_data, batch_size= 64, shuffle=True) #800 is the number of samples in train set\n",
    "# Execute the training loop\n",
    "for epoch in range(500): # increasing the epochs for effective training\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        pred = model(X_batch)\n",
    "        loss = criterion(pred, y_batch)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 50 == 0: # printing after every 100 epochs\n",
    "        train_loss = criterion(model(X_train_tensor), y_train_tensor).item()\n",
    "        # print(epoch,': ', train_loss)\n",
    "        test_loss = criterion(model(X_test_tensor), y_test_tensor).item()\n",
    "        print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05feccd2-527c-4b05-98a4-bda19bd3848e",
   "metadata": {},
   "source": [
    "### Gradient Descent with Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "203e39cb-c597-47cb-b111-5b14066b4759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialising model weights\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(2, 1)\n",
    ")\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "311b5de5-ec08-4642-8fa6-5f8c1710e53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: Train Loss: 0.3584, Test Loss: 0.4156\n",
      "Epoch 100: Train Loss: 0.3460, Test Loss: 0.4055\n",
      "Epoch 150: Train Loss: 0.3442, Test Loss: 0.4040\n",
      "Epoch 200: Train Loss: 0.3435, Test Loss: 0.4029\n",
      "Epoch 250: Train Loss: 0.3430, Test Loss: 0.4025\n",
      "Epoch 300: Train Loss: 0.3426, Test Loss: 0.4022\n",
      "Epoch 350: Train Loss: 0.3423, Test Loss: 0.4020\n",
      "Epoch 400: Train Loss: 0.3420, Test Loss: 0.4018\n",
      "Epoch 450: Train Loss: 0.3418, Test Loss: 0.4017\n",
      "Epoch 500: Train Loss: 0.3415, Test Loss: 0.4013\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_data, batch_size= 64, shuffle=True) #800 is the number of samples in train set\n",
    "# Execute the training loop\n",
    "for epoch in range(500): # increasing the epochs for effective training\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        pred = model(X_batch)\n",
    "        loss = criterion(pred, y_batch)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 50 == 0: # printing after every 100 epochs\n",
    "        train_loss = criterion(model(X_train_tensor), y_train_tensor).item()\n",
    "        # print(epoch,': ', train_loss)\n",
    "        test_loss = criterion(model(X_test_tensor), y_test_tensor).item()\n",
    "        print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008a1e62-f0c4-4ed3-867f-81c00a3f78bf",
   "metadata": {},
   "source": [
    "### Nesterov Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "efca6b4b-27d5-4403-8eb5-f56be3f1aee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialising model weights\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(2, 1)\n",
    ")\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "673a07d1-3384-4584-b44e-666e0b5707e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: Train Loss: 0.3526, Test Loss: 0.4035\n",
      "Epoch 100: Train Loss: 0.3469, Test Loss: 0.4014\n",
      "Epoch 150: Train Loss: 0.3458, Test Loss: 0.4016\n",
      "Epoch 200: Train Loss: 0.3452, Test Loss: 0.4019\n",
      "Epoch 250: Train Loss: 0.3446, Test Loss: 0.4018\n",
      "Epoch 300: Train Loss: 0.3441, Test Loss: 0.4009\n",
      "Epoch 350: Train Loss: 0.3437, Test Loss: 0.4011\n",
      "Epoch 400: Train Loss: 0.3433, Test Loss: 0.4005\n",
      "Epoch 450: Train Loss: 0.3429, Test Loss: 0.4008\n",
      "Epoch 500: Train Loss: 0.3426, Test Loss: 0.4003\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_data, batch_size= 64, shuffle=True) #800 is the number of samples in train set\n",
    "# Execute the training loop\n",
    "for epoch in range(500): # increasing the epochs for effective training\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        pred = model(X_batch)\n",
    "        loss = criterion(pred, y_batch)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 50 == 0: # printing after every 100 epochs\n",
    "        train_loss = criterion(model(X_train_tensor), y_train_tensor).item()\n",
    "        # print(epoch,': ', train_loss)\n",
    "        test_loss = criterion(model(X_test_tensor), y_test_tensor).item()\n",
    "        print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fe789b-346c-4331-a566-a1e066ec92b6",
   "metadata": {},
   "source": [
    "### Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9cd0b6ab-219c-4fc2-9371-318ad9c7d3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialising model weights\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(2, 1)\n",
    ")\n",
    "optimizer = optim.Adagrad(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "023d8f9b-47ff-4b58-9bae-88ad0653e397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: Train Loss: 6.4846, Test Loss: 6.7470\n",
      "Epoch 100: Train Loss: 4.5405, Test Loss: 4.7661\n",
      "Epoch 150: Train Loss: 3.2570, Test Loss: 3.4549\n",
      "Epoch 200: Train Loss: 2.3882, Test Loss: 2.5641\n",
      "Epoch 250: Train Loss: 1.7883, Test Loss: 1.9460\n",
      "Epoch 300: Train Loss: 1.3680, Test Loss: 1.5101\n",
      "Epoch 350: Train Loss: 1.0716, Test Loss: 1.2003\n",
      "Epoch 400: Train Loss: 0.8614, Test Loss: 0.9784\n",
      "Epoch 450: Train Loss: 0.7124, Test Loss: 0.8193\n",
      "Epoch 500: Train Loss: 0.6072, Test Loss: 0.7054\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_data, batch_size= 64, shuffle=True) #800 is the number of samples in train set\n",
    "# Execute the training loop\n",
    "for epoch in range(500): # increasing the epochs for effective training\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        pred = model(X_batch)\n",
    "        loss = criterion(pred, y_batch)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 50 == 0: # printing after every 100 epochs\n",
    "        train_loss = criterion(model(X_train_tensor), y_train_tensor).item()\n",
    "        # print(epoch,': ', train_loss)\n",
    "        test_loss = criterion(model(X_test_tensor), y_test_tensor).item()\n",
    "        print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafca420-91f1-4810-bac6-5c61e073ca3c",
   "metadata": {},
   "source": [
    "### RMS Prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e3d8aaa-ffd2-4c22-8092-8b9de52b3c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialising model weights\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(2, 1)\n",
    ")\n",
    "optimizer = optim.RMSprop(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c2ae365-c765-4368-a76e-8dde17837000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: Train Loss: 0.3417, Test Loss: 0.4007\n",
      "Epoch 100: Train Loss: 0.3389, Test Loss: 0.4011\n",
      "Epoch 150: Train Loss: 0.3389, Test Loss: 0.4017\n",
      "Epoch 200: Train Loss: 0.3396, Test Loss: 0.3983\n",
      "Epoch 250: Train Loss: 0.3391, Test Loss: 0.4042\n",
      "Epoch 300: Train Loss: 0.3372, Test Loss: 0.4005\n",
      "Epoch 350: Train Loss: 0.3389, Test Loss: 0.4046\n",
      "Epoch 400: Train Loss: 0.3366, Test Loss: 0.3985\n",
      "Epoch 450: Train Loss: 0.3367, Test Loss: 0.3987\n",
      "Epoch 500: Train Loss: 0.3390, Test Loss: 0.3989\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_data, batch_size= 64, shuffle=True) #800 is the number of samples in train set\n",
    "# Execute the training loop\n",
    "for epoch in range(500): # increasing the epochs for effective training\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        pred = model(X_batch)\n",
    "        loss = criterion(pred, y_batch)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 50 == 0: # printing after every 100 epochs\n",
    "        train_loss = criterion(model(X_train_tensor), y_train_tensor).item()\n",
    "        # print(epoch,': ', train_loss)\n",
    "        test_loss = criterion(model(X_test_tensor), y_test_tensor).item()\n",
    "        print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c5cbae-5b62-4a9e-a2ba-7bc8bfbfeccd",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e3a6ce4-e394-4c72-857c-bc419f69aea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialising model weights\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(2, 1)\n",
    ")\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b11f3f43-4198-4124-ab1a-d8f88ac8d494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: Train Loss: 1.3292, Test Loss: 1.4643\n",
      "Epoch 100: Train Loss: 0.4310, Test Loss: 0.4997\n",
      "Epoch 150: Train Loss: 0.3758, Test Loss: 0.4283\n",
      "Epoch 200: Train Loss: 0.3711, Test Loss: 0.4220\n",
      "Epoch 250: Train Loss: 0.3674, Test Loss: 0.4190\n",
      "Epoch 300: Train Loss: 0.3632, Test Loss: 0.4154\n",
      "Epoch 350: Train Loss: 0.3589, Test Loss: 0.4125\n",
      "Epoch 400: Train Loss: 0.3550, Test Loss: 0.4097\n",
      "Epoch 450: Train Loss: 0.3516, Test Loss: 0.4079\n",
      "Epoch 500: Train Loss: 0.3490, Test Loss: 0.4066\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_data, batch_size= 64, shuffle=True) #800 is the number of samples in train set\n",
    "# Execute the training loop\n",
    "for epoch in range(500): # increasing the epochs for effective training\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        pred = model(X_batch)\n",
    "        loss = criterion(pred, y_batch)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 50 == 0: # printing after every 100 epochs\n",
    "        train_loss = criterion(model(X_train_tensor), y_train_tensor).item()\n",
    "        # print(epoch,': ', train_loss)\n",
    "        test_loss = criterion(model(X_test_tensor), y_test_tensor).item()\n",
    "        print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44964fbe-d268-4590-a75b-27e1d65580b0",
   "metadata": {},
   "source": [
    "RMS Prop gave the best results so far"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
